{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies"
      ],
      "metadata": {
        "id": "e5aJ4smScHGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "!pip install shapely"
      ],
      "metadata": {
        "id": "4OaZT6o8cEXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a2e6ab-5342-4b15-fb1a-6edb9d8caafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.26.4)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (3.7.0)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.16.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (2.0.6)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un/zip Files"
      ],
      "metadata": {
        "id": "zCrovlvEbcJ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar3IIq2RbBro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cfa9bd-b729-4f9a-883f-96a7431d645a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  mean-sea-level-pressure-2020.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of mean-sea-level-pressure-2020.zip or\n",
            "        mean-sea-level-pressure-2020.zip.zip, and cannot find mean-sea-level-pressure-2020.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!unzip mean-sea-level-pressure-2020.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/merged.zip /content/merged"
      ],
      "metadata": {
        "id": "HiVZRxQfbUR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing ERA5 to create polygons with attributes"
      ],
      "metadata": {
        "id": "qa9P26tZb2y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "\n",
        "filename = 'data_stream-oper.nc'\n",
        "\n",
        "# Load the NetCDF dataset\n",
        "ds = xr.open_dataset(filename)\n",
        "ds"
      ],
      "metadata": {
        "id": "sn3OmfQib2El",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "b0e77ab3-38d9-49bf-d4fb-c3a878fa5318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<xarray.Dataset> Size: 430MB\n",
              "Dimensions:     (valid_time: 856, latitude: 261, longitude: 481)\n",
              "Coordinates:\n",
              "    number      int64 8B ...\n",
              "  * valid_time  (valid_time) datetime64[ns] 7kB 2020-05-01 ... 2020-11-30T18:...\n",
              "  * latitude    (latitude) float64 2kB 65.0 64.75 64.5 64.25 ... 0.5 0.25 0.0\n",
              "  * longitude   (longitude) float64 4kB -120.0 -119.8 -119.5 ... -0.5 -0.25 0.0\n",
              "    expver      (valid_time) <U4 14kB ...\n",
              "Data variables:\n",
              "    msl         (valid_time, latitude, longitude) float32 430MB ...\n",
              "Attributes:\n",
              "    GRIB_centre:             ecmf\n",
              "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
              "    GRIB_subCentre:          0\n",
              "    Conventions:             CF-1.7\n",
              "    institution:             European Centre for Medium-Range Weather Forecasts\n",
              "    history:                 2024-11-19T23:30 GRIB to CDM+CF via cfgrib-0.9.1..."
            ],
            "text/html": [
              "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
              "<defs>\n",
              "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
              "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
              "</symbol>\n",
              "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
              "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
              "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
              "</symbol>\n",
              "</defs>\n",
              "</svg>\n",
              "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
              " *\n",
              " */\n",
              "\n",
              ":root {\n",
              "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
              "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
              "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
              "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
              "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
              "  --xr-background-color: var(--jp-layout-color0, white);\n",
              "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
              "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
              "}\n",
              "\n",
              "html[theme=dark],\n",
              "html[data-theme=dark],\n",
              "body[data-theme=dark],\n",
              "body.vscode-dark {\n",
              "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
              "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
              "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
              "  --xr-border-color: #1F1F1F;\n",
              "  --xr-disabled-color: #515151;\n",
              "  --xr-background-color: #111111;\n",
              "  --xr-background-color-row-even: #111111;\n",
              "  --xr-background-color-row-odd: #313131;\n",
              "}\n",
              "\n",
              ".xr-wrap {\n",
              "  display: block !important;\n",
              "  min-width: 300px;\n",
              "  max-width: 700px;\n",
              "}\n",
              "\n",
              ".xr-text-repr-fallback {\n",
              "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-header {\n",
              "  padding-top: 6px;\n",
              "  padding-bottom: 6px;\n",
              "  margin-bottom: 4px;\n",
              "  border-bottom: solid 1px var(--xr-border-color);\n",
              "}\n",
              "\n",
              ".xr-header > div,\n",
              ".xr-header > ul {\n",
              "  display: inline;\n",
              "  margin-top: 0;\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-obj-type,\n",
              ".xr-array-name {\n",
              "  margin-left: 2px;\n",
              "  margin-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-obj-type {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-sections {\n",
              "  padding-left: 0 !important;\n",
              "  display: grid;\n",
              "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
              "}\n",
              "\n",
              ".xr-section-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-section-item input {\n",
              "  display: inline-block;\n",
              "  opacity: 0;\n",
              "}\n",
              "\n",
              ".xr-section-item input + label {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label {\n",
              "  cursor: pointer;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-item input:focus + label {\n",
              "  border: 2px solid var(--xr-font-color0);\n",
              "}\n",
              "\n",
              ".xr-section-item input:enabled + label:hover {\n",
              "  color: var(--xr-font-color0);\n",
              "}\n",
              "\n",
              ".xr-section-summary {\n",
              "  grid-column: 1;\n",
              "  color: var(--xr-font-color2);\n",
              "  font-weight: 500;\n",
              "}\n",
              "\n",
              ".xr-section-summary > span {\n",
              "  display: inline-block;\n",
              "  padding-left: 0.5em;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label {\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in + label:before {\n",
              "  display: inline-block;\n",
              "  content: '►';\n",
              "  font-size: 11px;\n",
              "  width: 15px;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:disabled + label:before {\n",
              "  color: var(--xr-disabled-color);\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label:before {\n",
              "  content: '▼';\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked + label > span {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-section-summary,\n",
              ".xr-section-inline-details {\n",
              "  padding-top: 4px;\n",
              "  padding-bottom: 4px;\n",
              "}\n",
              "\n",
              ".xr-section-inline-details {\n",
              "  grid-column: 2 / -1;\n",
              "}\n",
              "\n",
              ".xr-section-details {\n",
              "  display: none;\n",
              "  grid-column: 1 / -1;\n",
              "  margin-bottom: 5px;\n",
              "}\n",
              "\n",
              ".xr-section-summary-in:checked ~ .xr-section-details {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-array-wrap {\n",
              "  grid-column: 1 / -1;\n",
              "  display: grid;\n",
              "  grid-template-columns: 20px auto;\n",
              "}\n",
              "\n",
              ".xr-array-wrap > label {\n",
              "  grid-column: 1;\n",
              "  vertical-align: top;\n",
              "}\n",
              "\n",
              ".xr-preview {\n",
              "  color: var(--xr-font-color3);\n",
              "}\n",
              "\n",
              ".xr-array-preview,\n",
              ".xr-array-data {\n",
              "  padding: 0 5px !important;\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-array-data,\n",
              ".xr-array-in:checked ~ .xr-array-preview {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              ".xr-array-in:checked ~ .xr-array-data,\n",
              ".xr-array-preview {\n",
              "  display: inline-block;\n",
              "}\n",
              "\n",
              ".xr-dim-list {\n",
              "  display: inline-block !important;\n",
              "  list-style: none;\n",
              "  padding: 0 !important;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list li {\n",
              "  display: inline-block;\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "}\n",
              "\n",
              ".xr-dim-list:before {\n",
              "  content: '(';\n",
              "}\n",
              "\n",
              ".xr-dim-list:after {\n",
              "  content: ')';\n",
              "}\n",
              "\n",
              ".xr-dim-list li:not(:last-child):after {\n",
              "  content: ',';\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-has-index {\n",
              "  font-weight: bold;\n",
              "}\n",
              "\n",
              ".xr-var-list,\n",
              ".xr-var-item {\n",
              "  display: contents;\n",
              "}\n",
              "\n",
              ".xr-var-item > div,\n",
              ".xr-var-item label,\n",
              ".xr-var-item > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-even);\n",
              "  margin-bottom: 0;\n",
              "}\n",
              "\n",
              ".xr-var-item > .xr-var-name:hover span {\n",
              "  padding-right: 5px;\n",
              "}\n",
              "\n",
              ".xr-var-list > li:nth-child(odd) > div,\n",
              ".xr-var-list > li:nth-child(odd) > label,\n",
              ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
              "  background-color: var(--xr-background-color-row-odd);\n",
              "}\n",
              "\n",
              ".xr-var-name {\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-var-dims {\n",
              "  grid-column: 2;\n",
              "}\n",
              "\n",
              ".xr-var-dtype {\n",
              "  grid-column: 3;\n",
              "  text-align: right;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-preview {\n",
              "  grid-column: 4;\n",
              "}\n",
              "\n",
              ".xr-index-preview {\n",
              "  grid-column: 2 / 5;\n",
              "  color: var(--xr-font-color2);\n",
              "}\n",
              "\n",
              ".xr-var-name,\n",
              ".xr-var-dims,\n",
              ".xr-var-dtype,\n",
              ".xr-preview,\n",
              ".xr-attrs dt {\n",
              "  white-space: nowrap;\n",
              "  overflow: hidden;\n",
              "  text-overflow: ellipsis;\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-var-name:hover,\n",
              ".xr-var-dims:hover,\n",
              ".xr-var-dtype:hover,\n",
              ".xr-attrs dt:hover {\n",
              "  overflow: visible;\n",
              "  width: auto;\n",
              "  z-index: 1;\n",
              "}\n",
              "\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  display: none;\n",
              "  background-color: var(--xr-background-color) !important;\n",
              "  padding-bottom: 5px !important;\n",
              "}\n",
              "\n",
              ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
              ".xr-var-data-in:checked ~ .xr-var-data,\n",
              ".xr-index-data-in:checked ~ .xr-index-data {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              ".xr-var-data > table {\n",
              "  float: right;\n",
              "}\n",
              "\n",
              ".xr-var-name span,\n",
              ".xr-var-data,\n",
              ".xr-index-name div,\n",
              ".xr-index-data,\n",
              ".xr-attrs {\n",
              "  padding-left: 25px !important;\n",
              "}\n",
              "\n",
              ".xr-attrs,\n",
              ".xr-var-attrs,\n",
              ".xr-var-data,\n",
              ".xr-index-data {\n",
              "  grid-column: 1 / -1;\n",
              "}\n",
              "\n",
              "dl.xr-attrs {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  display: grid;\n",
              "  grid-template-columns: 125px auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt,\n",
              ".xr-attrs dd {\n",
              "  padding: 0;\n",
              "  margin: 0;\n",
              "  float: left;\n",
              "  padding-right: 10px;\n",
              "  width: auto;\n",
              "}\n",
              "\n",
              ".xr-attrs dt {\n",
              "  font-weight: normal;\n",
              "  grid-column: 1;\n",
              "}\n",
              "\n",
              ".xr-attrs dt:hover span {\n",
              "  display: inline-block;\n",
              "  background: var(--xr-background-color);\n",
              "  padding-right: 10px;\n",
              "}\n",
              "\n",
              ".xr-attrs dd {\n",
              "  grid-column: 2;\n",
              "  white-space: pre-wrap;\n",
              "  word-break: break-all;\n",
              "}\n",
              "\n",
              ".xr-icon-database,\n",
              ".xr-icon-file-text2,\n",
              ".xr-no-icon {\n",
              "  display: inline-block;\n",
              "  vertical-align: middle;\n",
              "  width: 1em;\n",
              "  height: 1.5em !important;\n",
              "  stroke-width: 0;\n",
              "  stroke: currentColor;\n",
              "  fill: currentColor;\n",
              "}\n",
              "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 430MB\n",
              "Dimensions:     (valid_time: 856, latitude: 261, longitude: 481)\n",
              "Coordinates:\n",
              "    number      int64 8B ...\n",
              "  * valid_time  (valid_time) datetime64[ns] 7kB 2020-05-01 ... 2020-11-30T18:...\n",
              "  * latitude    (latitude) float64 2kB 65.0 64.75 64.5 64.25 ... 0.5 0.25 0.0\n",
              "  * longitude   (longitude) float64 4kB -120.0 -119.8 -119.5 ... -0.5 -0.25 0.0\n",
              "    expver      (valid_time) &lt;U4 14kB ...\n",
              "Data variables:\n",
              "    msl         (valid_time, latitude, longitude) float32 430MB ...\n",
              "Attributes:\n",
              "    GRIB_centre:             ecmf\n",
              "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
              "    GRIB_subCentre:          0\n",
              "    Conventions:             CF-1.7\n",
              "    institution:             European Centre for Medium-Range Weather Forecasts\n",
              "    history:                 2024-11-19T23:30 GRIB to CDM+CF via cfgrib-0.9.1...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-e622ee82-4eb6-4338-91f3-8e46daf319d6' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-e622ee82-4eb6-4338-91f3-8e46daf319d6' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>valid_time</span>: 856</li><li><span class='xr-has-index'>latitude</span>: 261</li><li><span class='xr-has-index'>longitude</span>: 481</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-010c2d42-2977-4c55-90d8-6d59e94150f3' class='xr-section-summary-in' type='checkbox'  checked><label for='section-010c2d42-2977-4c55-90d8-6d59e94150f3' class='xr-section-summary' >Coordinates: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>number</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-39e49f2a-44d7-499f-88ca-920d22950303' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-39e49f2a-44d7-499f-88ca-920d22950303' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-905d31b1-d487-4a53-be50-a7a43c732c72' class='xr-var-data-in' type='checkbox'><label for='data-905d31b1-d487-4a53-be50-a7a43c732c72' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>ensemble member numerical id</dd><dt><span>units :</span></dt><dd>1</dd><dt><span>standard_name :</span></dt><dd>realization</dd></dl></div><div class='xr-var-data'><pre>[1 values with dtype=int64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>valid_time</span></div><div class='xr-var-dims'>(valid_time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2020-05-01 ... 2020-11-30T18:00:00</div><input id='attrs-c41a5d38-5410-4250-adbe-5a540aca83f1' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c41a5d38-5410-4250-adbe-5a540aca83f1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-42b7ea2e-ab82-4d52-8da4-fbeb90906e30' class='xr-var-data-in' type='checkbox'><label for='data-42b7ea2e-ab82-4d52-8da4-fbeb90906e30' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time</dd><dt><span>standard_name :</span></dt><dd>time</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2020-05-01T00:00:00.000000000&#x27;, &#x27;2020-05-01T06:00:00.000000000&#x27;,\n",
              "       &#x27;2020-05-01T12:00:00.000000000&#x27;, ..., &#x27;2020-11-30T06:00:00.000000000&#x27;,\n",
              "       &#x27;2020-11-30T12:00:00.000000000&#x27;, &#x27;2020-11-30T18:00:00.000000000&#x27;],\n",
              "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>65.0 64.75 64.5 ... 0.5 0.25 0.0</div><input id='attrs-18c6ebc7-3483-479c-b869-03094b67c074' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-18c6ebc7-3483-479c-b869-03094b67c074' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-930e9055-1bce-416f-9b73-8fdde95f7d72' class='xr-var-data-in' type='checkbox'><label for='data-930e9055-1bce-416f-9b73-8fdde95f7d72' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>stored_direction :</span></dt><dd>decreasing</dd></dl></div><div class='xr-var-data'><pre>array([65.  , 64.75, 64.5 , ...,  0.5 ,  0.25,  0.  ])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-120.0 -119.8 -119.5 ... -0.25 0.0</div><input id='attrs-aeb05b3a-2589-404b-9d5f-4644a410db7a' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-aeb05b3a-2589-404b-9d5f-4644a410db7a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-292bc096-afcc-444c-bbea-57acc1ae9baa' class='xr-var-data-in' type='checkbox'><label for='data-292bc096-afcc-444c-bbea-57acc1ae9baa' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude</dd></dl></div><div class='xr-var-data'><pre>array([-120.  , -119.75, -119.5 , ...,   -0.5 ,   -0.25,    0.  ])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>expver</span></div><div class='xr-var-dims'>(valid_time)</div><div class='xr-var-dtype'>&lt;U4</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-47fd8b95-a50b-4d7a-880f-88d308c06541' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-47fd8b95-a50b-4d7a-880f-88d308c06541' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-abc7b164-4864-4868-bed3-53da7e70e67f' class='xr-var-data-in' type='checkbox'><label for='data-abc7b164-4864-4868-bed3-53da7e70e67f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[856 values with dtype=&lt;U4]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-5a751097-9422-4529-99d5-1f6855b7b4b3' class='xr-section-summary-in' type='checkbox'  checked><label for='section-5a751097-9422-4529-99d5-1f6855b7b4b3' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>msl</span></div><div class='xr-var-dims'>(valid_time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-40474c8d-b1d6-4296-805a-0c8fd4fabcc7' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-40474c8d-b1d6-4296-805a-0c8fd4fabcc7' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-984a931d-a2ef-41c5-b3e9-e142c656a877' class='xr-var-data-in' type='checkbox'><label for='data-984a931d-a2ef-41c5-b3e9-e142c656a877' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>GRIB_paramId :</span></dt><dd>151</dd><dt><span>GRIB_dataType :</span></dt><dd>an</dd><dt><span>GRIB_numberOfPoints :</span></dt><dd>125541</dd><dt><span>GRIB_typeOfLevel :</span></dt><dd>surface</dd><dt><span>GRIB_stepUnits :</span></dt><dd>1</dd><dt><span>GRIB_stepType :</span></dt><dd>instant</dd><dt><span>GRIB_gridType :</span></dt><dd>regular_ll</dd><dt><span>GRIB_uvRelativeToGrid :</span></dt><dd>0</dd><dt><span>GRIB_NV :</span></dt><dd>0</dd><dt><span>GRIB_Nx :</span></dt><dd>481</dd><dt><span>GRIB_Ny :</span></dt><dd>261</dd><dt><span>GRIB_cfName :</span></dt><dd>air_pressure_at_mean_sea_level</dd><dt><span>GRIB_cfVarName :</span></dt><dd>msl</dd><dt><span>GRIB_gridDefinitionDescription :</span></dt><dd>Latitude/Longitude Grid</dd><dt><span>GRIB_iDirectionIncrementInDegrees :</span></dt><dd>0.25</dd><dt><span>GRIB_iScansNegatively :</span></dt><dd>0</dd><dt><span>GRIB_jDirectionIncrementInDegrees :</span></dt><dd>0.25</dd><dt><span>GRIB_jPointsAreConsecutive :</span></dt><dd>0</dd><dt><span>GRIB_jScansPositively :</span></dt><dd>0</dd><dt><span>GRIB_latitudeOfFirstGridPointInDegrees :</span></dt><dd>65.0</dd><dt><span>GRIB_latitudeOfLastGridPointInDegrees :</span></dt><dd>0.0</dd><dt><span>GRIB_longitudeOfFirstGridPointInDegrees :</span></dt><dd>-120.0</dd><dt><span>GRIB_longitudeOfLastGridPointInDegrees :</span></dt><dd>0.0</dd><dt><span>GRIB_missingValue :</span></dt><dd>3.4028234663852886e+38</dd><dt><span>GRIB_name :</span></dt><dd>Mean sea level pressure</dd><dt><span>GRIB_shortName :</span></dt><dd>msl</dd><dt><span>GRIB_totalNumber :</span></dt><dd>0</dd><dt><span>GRIB_units :</span></dt><dd>Pa</dd><dt><span>long_name :</span></dt><dd>Mean sea level pressure</dd><dt><span>units :</span></dt><dd>Pa</dd><dt><span>standard_name :</span></dt><dd>air_pressure_at_mean_sea_level</dd><dt><span>GRIB_surface :</span></dt><dd>0.0</dd></dl></div><div class='xr-var-data'><pre>[107463096 values with dtype=float32]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-706f1939-8129-4ee2-9c70-5f78271ed840' class='xr-section-summary-in' type='checkbox'  ><label for='section-706f1939-8129-4ee2-9c70-5f78271ed840' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>valid_time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-5ee0d541-5eab-48a6-9e49-8edc066bfd12' class='xr-index-data-in' type='checkbox'/><label for='index-5ee0d541-5eab-48a6-9e49-8edc066bfd12' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2020-05-01 00:00:00&#x27;, &#x27;2020-05-01 06:00:00&#x27;,\n",
              "               &#x27;2020-05-01 12:00:00&#x27;, &#x27;2020-05-01 18:00:00&#x27;,\n",
              "               &#x27;2020-05-02 00:00:00&#x27;, &#x27;2020-05-02 06:00:00&#x27;,\n",
              "               &#x27;2020-05-02 12:00:00&#x27;, &#x27;2020-05-02 18:00:00&#x27;,\n",
              "               &#x27;2020-05-03 00:00:00&#x27;, &#x27;2020-05-03 06:00:00&#x27;,\n",
              "               ...\n",
              "               &#x27;2020-11-28 12:00:00&#x27;, &#x27;2020-11-28 18:00:00&#x27;,\n",
              "               &#x27;2020-11-29 00:00:00&#x27;, &#x27;2020-11-29 06:00:00&#x27;,\n",
              "               &#x27;2020-11-29 12:00:00&#x27;, &#x27;2020-11-29 18:00:00&#x27;,\n",
              "               &#x27;2020-11-30 00:00:00&#x27;, &#x27;2020-11-30 06:00:00&#x27;,\n",
              "               &#x27;2020-11-30 12:00:00&#x27;, &#x27;2020-11-30 18:00:00&#x27;],\n",
              "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;valid_time&#x27;, length=856, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>latitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-b9aaa2f3-9675-4b8a-acd1-e0c28b599652' class='xr-index-data-in' type='checkbox'/><label for='index-b9aaa2f3-9675-4b8a-acd1-e0c28b599652' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([ 65.0, 64.75,  64.5, 64.25,  64.0, 63.75,  63.5, 63.25,  63.0, 62.75,\n",
              "       ...\n",
              "        2.25,   2.0,  1.75,   1.5,  1.25,   1.0,  0.75,   0.5,  0.25,   0.0],\n",
              "      dtype=&#x27;float64&#x27;, name=&#x27;latitude&#x27;, length=261))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>longitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-87689d69-1475-4b66-818c-d61dc8afde33' class='xr-index-data-in' type='checkbox'/><label for='index-87689d69-1475-4b66-818c-d61dc8afde33' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([ -120.0, -119.75,  -119.5, -119.25,  -119.0, -118.75,  -118.5, -118.25,\n",
              "        -118.0, -117.75,\n",
              "       ...\n",
              "         -2.25,    -2.0,   -1.75,    -1.5,   -1.25,    -1.0,   -0.75,    -0.5,\n",
              "         -0.25,     0.0],\n",
              "      dtype=&#x27;float64&#x27;, name=&#x27;longitude&#x27;, length=481))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-ba761b1e-2690-4a79-80ad-707e9243e02b' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ba761b1e-2690-4a79-80ad-707e9243e02b' class='xr-section-summary' >Attributes: <span>(6)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>GRIB_centre :</span></dt><dd>ecmf</dd><dt><span>GRIB_centreDescription :</span></dt><dd>European Centre for Medium-Range Weather Forecasts</dd><dt><span>GRIB_subCentre :</span></dt><dd>0</dd><dt><span>Conventions :</span></dt><dd>CF-1.7</dd><dt><span>institution :</span></dt><dd>European Centre for Medium-Range Weather Forecasts</dd><dt><span>history :</span></dt><dd>2024-11-19T23:30 GRIB to CDM+CF via cfgrib-0.9.14.1/ecCodes-2.36.0 with {&quot;source&quot;: &quot;data.grib&quot;, &quot;filter_by_keys&quot;: {&quot;stream&quot;: [&quot;oper&quot;]}, &quot;encode_cf&quot;: [&quot;parameter&quot;, &quot;time&quot;, &quot;geography&quot;, &quot;vertical&quot;]}</dd></dl></div></li></ul></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the attribute dimension\n",
        "#ds = ds.squeeze(dim='msl')\n",
        "\n",
        "# Round latitude and longitude to the nearest degree\n",
        "ds = ds.assign_coords(\n",
        "    lat_rounded=ds.latitude.round(0),\n",
        "    lon_rounded=ds.longitude.round(0)\n",
        ")\n",
        "# Group by the rounded coordinates and compute the mean temperature\n",
        "ds_grouped = ds.groupby(['lat_rounded', 'lon_rounded']).mean()\n",
        "print(ds_grouped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrwLxHR6scZ8",
        "outputId": "71e4e263-4f49-4f34-dbc0-4b01283863d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<xarray.Dataset> Size: 708B\n",
            "Dimensions:      (lat_rounded: 5, lon_rounded: 5, valid_time: 5)\n",
            "Coordinates:\n",
            "  * lat_rounded  (lat_rounded) float64 40B 0.0 1.0 2.0 3.0 4.0\n",
            "  * lon_rounded  (lon_rounded) float64 40B -120.0 -119.0 -118.0 -117.0 -116.0\n",
            "    number       int64 8B 0\n",
            "  * valid_time   (valid_time) datetime64[ns] 40B 2020-05-01 ... 2020-05-02\n",
            "    expver       (valid_time) <U4 80B '0001' '0001' '0001' '0001' '0001'\n",
            "Data variables:\n",
            "    msl          (valid_time, lat_rounded, lon_rounded) float32 500B 1.009e+0...\n",
            "Attributes:\n",
            "    GRIB_centre:             ecmf\n",
            "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
            "    GRIB_subCentre:          0\n",
            "    Conventions:             CF-1.7\n",
            "    institution:             European Centre for Medium-Range Weather Forecasts\n",
            "    history:                 2024-11-19T23:30 GRIB to CDM+CF via cfgrib-0.9.1...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the grouped dataset to a DataFrame for tabular display\n",
        "df_grouped = ds_grouped.to_dataframe().reset_index()\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df_grouped[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSDCVKr_tVgw",
        "outputId": "c17232d2-6f96-4b07-90a5-8d4ad0ef16b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   lat_rounded  lon_rounded          valid_time            msl  number expver\n",
            "0          0.0       -120.0 2020-05-01 00:00:00  100877.593750       0   0001\n",
            "1          0.0       -120.0 2020-05-01 06:00:00  101223.148438       0   0001\n",
            "2          0.0       -120.0 2020-05-01 12:00:00  100960.304688       0   0001\n",
            "3          0.0       -120.0 2020-05-01 18:00:00  101278.570312       0   0001\n",
            "4          0.0       -120.0 2020-05-02 00:00:00  100917.718750       0   0001\n",
            "5          0.0       -120.0 2020-05-02 06:00:00  101290.343750       0   0001\n",
            "6          0.0       -120.0 2020-05-02 12:00:00  101031.335938       0   0001\n",
            "7          0.0       -120.0 2020-05-02 18:00:00  101348.242188       0   0001\n",
            "8          0.0       -120.0 2020-05-03 00:00:00  100988.062500       0   0001\n",
            "9          0.0       -120.0 2020-05-03 06:00:00  101294.656250       0   0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Polygon\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Union, List, Optional, Dict\n",
        "\n",
        "def create_grid_cell_polygon(longitude: float, latitude: float, cell_size: float = 1.0) -> Polygon:\n",
        "    \"\"\"\n",
        "    Create a square grid cell centered at (longitude, latitude) with given cell size.\n",
        "\n",
        "    Parameters:\n",
        "        longitude (float): Center point longitude\n",
        "        latitude (float): Center point latitude\n",
        "        cell_size (float): Size of the grid cell in degrees\n",
        "\n",
        "    Returns:\n",
        "        Polygon: Square polygon representing the grid cell\n",
        "    \"\"\"\n",
        "    half_size = cell_size / 2\n",
        "    return Polygon([\n",
        "        (longitude - half_size, latitude - half_size),  # bottom left\n",
        "        (longitude - half_size, latitude + half_size),  # top left\n",
        "        (longitude + half_size, latitude + half_size),  # top right\n",
        "        (longitude + half_size, latitude - half_size),  # bottom right\n",
        "        (longitude - half_size, latitude - half_size)   # back to start\n",
        "    ])\n",
        "\n",
        "def process_era5_grid(\n",
        "    era5_data: pd.DataFrame,\n",
        "    time_column: str = 'valid_time',\n",
        "    lat_column: str = 'latitude',\n",
        "    lon_column: str = 'longitude',\n",
        "    value_column: str = 'msl',\n",
        "    grid_cell_size: float = 1.0,\n",
        "    simplify_tolerance: Optional[float] = 0.01,\n",
        "    round_decimals: Optional[int] = 1,\n",
        "    output_filename_pattern: str = 'msl_{}.geojson',\n",
        "    crs: str = \"EPSG:4326\"\n",
        ")  -> Dict[datetime, Dict]:\n",
        "    \"\"\"\n",
        "    Convert ERA5 gridded data into GeoJSON files, one for each timestamp.\n",
        "\n",
        "    Parameters:\n",
        "        era5_data (pd.DataFrame): ERA5 data with columns for time, lat, lon, and values\n",
        "        time_column (str): Name of timestamp column\n",
        "        lat_column (str): Name of latitude column\n",
        "        lon_column (str): Name of longitude column\n",
        "        value_column (str): Name of data value column (e.g., temperature, pressure)\n",
        "        grid_cell_size (float): Size of each grid cell in degrees\n",
        "        simplify_tolerance (float): Tolerance for simplifying polygon geometries\n",
        "        round_decimals (int): Number of decimal places to round values\n",
        "        output_filename_pattern (str): Pattern for output files (e.g., 'temperature_{}.geojson')\n",
        "        crs (str): Coordinate reference system\n",
        "     Returns:\n",
        "        Dict[str, Dict]: Dictionary mapping timestamp strings to GeoJSON objects\n",
        "    \"\"\"\n",
        "    # Initialize results dictionary\n",
        "    geojson_results = {}\n",
        "\n",
        "    # Ensure timestamp column is datetime\n",
        "    if not pd.api.types.is_datetime64_any_dtype(era5_data[time_column]):\n",
        "        era5_data[time_column] = pd.to_datetime(era5_data[time_column])\n",
        "\n",
        "    # Get list of unique timestamps\n",
        "    unique_timestamps = sorted(era5_data[time_column].unique())\n",
        "\n",
        "    # Process each timestamp separately\n",
        "    for timestamp in unique_timestamps:\n",
        "        print(f\"Processing data for {timestamp}...\")\n",
        "\n",
        "        # Extract data for this timestamp\n",
        "        current_timestamp_data = era5_data[era5_data[time_column] == timestamp].copy()\n",
        "\n",
        "        # Get unique coordinate pairs for this timestamp\n",
        "        unique_coordinates = current_timestamp_data[[lat_column, lon_column]].drop_duplicates()\n",
        "\n",
        "        # Initialize lists to store our processed data\n",
        "        grid_cells = []         # Will store the polygon for each grid cell\n",
        "        grid_values = []        # Will store the data value for each cell\n",
        "        grid_latitudes = []     # Will store the latitude of each cell\n",
        "        grid_longitudes = []    # Will store the longitude of each cell\n",
        "\n",
        "        # Process each unique coordinate pair\n",
        "        total_coords = len(unique_coordinates)\n",
        "        for idx, coordinate in unique_coordinates.iterrows():\n",
        "            # Extract the latitude and longitude\n",
        "            latitude = coordinate[lat_column]\n",
        "            longitude = coordinate[lon_column]\n",
        "\n",
        "            # Find the matching data value for this location\n",
        "            matching_data = current_timestamp_data[\n",
        "                (current_timestamp_data[lat_column] == latitude) &\n",
        "                (current_timestamp_data[lon_column] == longitude)\n",
        "            ]\n",
        "\n",
        "            # Get the data value (temperature, pressure, etc.)\n",
        "            data_value = matching_data[value_column].iloc[0]\n",
        "\n",
        "            # Skip if the value is missing\n",
        "            if pd.isna(data_value):\n",
        "                continue\n",
        "\n",
        "            # Create a polygon representing this grid cell\n",
        "            grid_cell = create_grid_cell_polygon(\n",
        "                longitude=longitude,\n",
        "                latitude=latitude,\n",
        "                cell_size=grid_cell_size\n",
        "            )\n",
        "\n",
        "            # Store all information for this grid cell\n",
        "            grid_cells.append(grid_cell)\n",
        "            grid_values.append(data_value)\n",
        "            grid_latitudes.append(latitude)\n",
        "            grid_longitudes.append(longitude)\n",
        "\n",
        "\n",
        "        # Create GeoDataFrame from processed data\n",
        "        geodata = gpd.GeoDataFrame(\n",
        "            {\n",
        "                time_column: timestamp,\n",
        "                value_column: grid_values,\n",
        "                lat_column: grid_latitudes,\n",
        "                lon_column: grid_longitudes,\n",
        "                'geometry': grid_cells\n",
        "            },\n",
        "            crs=crs\n",
        "        )\n",
        "\n",
        "        # Simplify the polygons if requested\n",
        "        if simplify_tolerance is not None:\n",
        "            geodata['geometry'] = geodata['geometry'].simplify(\n",
        "                tolerance=simplify_tolerance,\n",
        "                preserve_topology=True\n",
        "            )\n",
        "\n",
        "        # Round the data values if requested\n",
        "        if round_decimals is not None:\n",
        "            geodata[value_column] = geodata[value_column].round(round_decimals)\n",
        "\n",
        "        # Convert to GeoJSON\n",
        "        geojson_data = geodata.__geo_interface__\n",
        "\n",
        "        # Store in results dictionary\n",
        "        geojson_results[timestamp] = geojson_data\n",
        "\n",
        "    return geojson_results"
      ],
      "metadata": {
        "id": "NbGdq9-5cAgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geojson_results = process_era5_grid(\n",
        "    era5_data=df_grouped,\n",
        "    time_column='valid_time',\n",
        "    lat_column='lat_rounded',\n",
        "    lon_column='lon_rounded',\n",
        "    value_column='msl',\n",
        "    grid_cell_size=1.0,\n",
        "    round_decimals=1,\n",
        "    simplify_tolerance = 0.01,\n",
        "    output_filename_pattern = 'msl/msl_{}.geojson',\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "geojson_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "BPparbVj1-7K",
        "outputId": "f6738d90-9a9c-4390-b5af-d69b4f1885e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data for 2020-05-01 00:00:00...\n",
            "Processing data for 2020-05-01 06:00:00...\n",
            "Processing data for 2020-05-01 12:00:00...\n",
            "Processing data for 2020-05-01 18:00:00...\n",
            "Processing data for 2020-05-02 00:00:00...\n",
            "Processing data for 2020-05-02 06:00:00...\n",
            "Processing data for 2020-05-02 12:00:00...\n",
            "Processing data for 2020-05-02 18:00:00...\n",
            "Processing data for 2020-05-03 00:00:00...\n",
            "Processing data for 2020-05-03 06:00:00...\n",
            "Processing data for 2020-05-03 12:00:00...\n",
            "Processing data for 2020-05-03 18:00:00...\n",
            "Processing data for 2020-05-04 00:00:00...\n",
            "Processing data for 2020-05-04 06:00:00...\n",
            "Processing data for 2020-05-04 12:00:00...\n",
            "Processing data for 2020-05-04 18:00:00...\n",
            "Processing data for 2020-05-05 00:00:00...\n",
            "Processing data for 2020-05-05 06:00:00...\n",
            "Processing data for 2020-05-05 12:00:00...\n",
            "Processing data for 2020-05-05 18:00:00...\n",
            "Processing data for 2020-05-06 00:00:00...\n",
            "Processing data for 2020-05-06 06:00:00...\n",
            "Processing data for 2020-05-06 12:00:00...\n",
            "Processing data for 2020-05-06 18:00:00...\n",
            "Processing data for 2020-05-07 00:00:00...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-bfafed35ec57>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m geojson_results = process_era5_grid(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mera5_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_grouped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtime_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid_time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlat_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lat_rounded'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlon_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lon_rounded'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-40f30407dff5>\u001b[0m in \u001b[0;36mprocess_era5_grid\u001b[0;34m(era5_data, time_column, lat_column, lon_column, value_column, grid_cell_size, simplify_tolerance, round_decimals, output_filename_pattern, crs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Find the matching data value for this location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             matching_data = current_timestamp_data[\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mcurrent_timestamp_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlat_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlatitude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mcurrent_timestamp_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlon_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlongitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6119\u001b[0m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_logical_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   6228\u001b[0m         \u001b[0;31m# TODO: result should always be ArrayLike, but this fails for some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6229\u001b[0m         \u001b[0;31m#  JSONArray tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6230\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6231\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6232\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Polygon\n",
        "import numpy as np\n",
        "from typing import Dict, Optional\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def validate_era5_data(\n",
        "    data: pd.DataFrame,\n",
        "    time_column: str,\n",
        "    lat_column: str,\n",
        "    lon_column: str,\n",
        "    value_column: str\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Comprehensive validation of ERA5 input data.\n",
        "\n",
        "    Args:\n",
        "        data: Input DataFrame\n",
        "        time_column: Name of timestamp column\n",
        "        lat_column: Name of latitude column\n",
        "        lon_column: Name of longitude column\n",
        "        value_column: Name of value column\n",
        "\n",
        "    Returns:\n",
        "        bool: True if data passes all validations\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If input is not a DataFrame\n",
        "        ValueError: If validation fails\n",
        "    \"\"\"\n",
        "    # Type checking\n",
        "    if not isinstance(data, pd.DataFrame):\n",
        "        raise TypeError(f\"Expected pandas DataFrame, got {type(data)}\")\n",
        "\n",
        "    # Check required columns\n",
        "    missing_columns = [\n",
        "        col for col in [time_column, lat_column, lon_column, value_column]\n",
        "        if col not in data.columns\n",
        "    ]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "    # Validate data types\n",
        "    try:\n",
        "        pd.to_datetime(data[time_column])\n",
        "    except (TypeError, ValueError):\n",
        "        raise ValueError(\"Invalid timestamp column format\")\n",
        "\n",
        "    # Latitude range check\n",
        "    lat_valid = data[lat_column].apply(lambda x: -90 <= x <= 90)\n",
        "    if not lat_valid.all():\n",
        "        invalid_lats = data[~lat_valid][lat_column]\n",
        "        print(f\"WARNING: Invalid latitude values found: {invalid_lats}\")\n",
        "        raise ValueError(\"Latitude values must be between -90 and 90 degrees\")\n",
        "\n",
        "    # Longitude range check\n",
        "    lon_valid = data[lon_column].apply(lambda x: -180 <= x <= 180)\n",
        "    if not lon_valid.all():\n",
        "        invalid_lons = data[~lon_valid][lon_column]\n",
        "        print(f\"WARNING: Invalid longitude values found: {invalid_lons}\")\n",
        "        raise ValueError(\"Longitude values must be between -180 and 180 degrees\")\n",
        "\n",
        "    # Check for excessive NaN values\n",
        "    nan_ratio = data[value_column].isna().mean()\n",
        "    if nan_ratio > 0.5:\n",
        "        print(f\"WARNING: High NaN ratio in value column: {nan_ratio:.2%}\")\n",
        "        raise ValueError(\"Too many NaN values in the value column\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def process_era5_grid_optimized(\n",
        "    era5_data: pd.DataFrame,\n",
        "    time_column: str = 'valid_time',\n",
        "    lat_column: str = 'latitude',\n",
        "    lon_column: str = 'longitude',\n",
        "    value_column: str = 'msl',\n",
        "    grid_cell_size: float = 1.0,\n",
        "    simplify_tolerance: Optional[float] = 0.01,\n",
        "    round_decimals: Optional[int] = 1,\n",
        "    crs: str = \"EPSG:4326\",\n",
        "    agg_method: str = 'first'\n",
        ") -> Dict[datetime, Dict]:\n",
        "    \"\"\"\n",
        "    Optimized ERA5 grid data processor with enhanced error handling.\n",
        "\n",
        "    Args:\n",
        "        era5_data: Input DataFrame with grid data\n",
        "        time_column: Timestamp column name\n",
        "        lat_column: Latitude column name\n",
        "        lon_column: Longitude column name\n",
        "        value_column: Value column name\n",
        "        grid_cell_size: Grid cell size in degrees\n",
        "        simplify_tolerance: Geometry simplification tolerance\n",
        "        round_decimals: Decimal places for value rounding\n",
        "        crs: Coordinate reference system\n",
        "        agg_method: Aggregation method for pivot table\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of timestamps to GeoJSON data\n",
        "    \"\"\"\n",
        "    # Validate input data\n",
        "    validate_era5_data(\n",
        "        era5_data,\n",
        "        time_column,\n",
        "        lat_column,\n",
        "        lon_column,\n",
        "        value_column\n",
        "    )\n",
        "\n",
        "    # Parameter validation\n",
        "    if not isinstance(grid_cell_size, (int, float)) or grid_cell_size <= 0:\n",
        "        raise ValueError(\"Grid cell size must be a positive number\")\n",
        "\n",
        "    # Parameter type checking\n",
        "    if simplify_tolerance is not None and (not isinstance(simplify_tolerance, (int, float)) or simplify_tolerance < 0):\n",
        "        raise ValueError(\"Simplify tolerance must be a non-negative number\")\n",
        "\n",
        "    geojson_results: Dict[datetime, Dict] = {}\n",
        "\n",
        "    # Convert timestamp column to datetime\n",
        "    era5_data[time_column] = pd.to_datetime(era5_data[time_column])\n",
        "    unique_timestamps = era5_data[time_column].unique()\n",
        "\n",
        "    # Efficient pivot table creation with custom aggregation\n",
        "    print(f\"Processing data with {len(unique_timestamps)} unique timestamps...\")\n",
        "\n",
        "    try:\n",
        "        pivoted_data = (\n",
        "            era5_data.pivot_table(\n",
        "                index=[lat_column, lon_column],\n",
        "                columns=time_column,\n",
        "                values=value_column,\n",
        "                aggfunc=agg_method\n",
        "            )\n",
        "            .dropna(how='all')\n",
        "            .reset_index()\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Pivot table creation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Vectorized polygon creation\n",
        "    half_size = grid_cell_size / 2\n",
        "\n",
        "    def safe_create_polygon(lat: float, lon: float) -> Polygon:\n",
        "        \"\"\"\n",
        "        Create polygon with additional error checking.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return Polygon([\n",
        "                (lon - half_size, lat - half_size),\n",
        "                (lon - half_size, lat + half_size),\n",
        "                (lon + half_size, lat + half_size),\n",
        "                (lon + half_size, lat - half_size),\n",
        "                (lon - half_size, lat - half_size)\n",
        "            ])\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Polygon creation failed for lat={lat}, lon={lon}: {e}\")\n",
        "            raise\n",
        "\n",
        "    pivoted_data['geometry'] = pivoted_data.apply(\n",
        "        lambda row: safe_create_polygon(row[lat_column], row[lon_column]),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Process timestamps\n",
        "    for timestamp in tqdm(unique_timestamps, desc=\"Processing timestamps\"):\n",
        "        # Extract data for current timestamp\n",
        "        timestamp_data = pivoted_data[[\n",
        "            lat_column,\n",
        "            lon_column,\n",
        "            'geometry',\n",
        "            timestamp\n",
        "        ]].copy()\n",
        "        timestamp_data.columns = [lat_column, lon_column, 'geometry', value_column]\n",
        "\n",
        "        # Robust NaN handling\n",
        "        timestamp_data.dropna(subset=[value_column], inplace=True)\n",
        "\n",
        "        # Round values if specified\n",
        "        if round_decimals is not None:\n",
        "            timestamp_data[value_column] = timestamp_data[value_column].round(round_decimals)\n",
        "\n",
        "        # Create GeoDataFrame with error handling\n",
        "        try:\n",
        "            geodata = gpd.GeoDataFrame(\n",
        "                timestamp_data,\n",
        "                geometry='geometry',\n",
        "                crs=crs\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: GeoDataFrame creation failed for {timestamp}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Geometry simplification with topology preservation\n",
        "        if simplify_tolerance:\n",
        "            try:\n",
        "                geodata['geometry'] = geodata['geometry'].simplify(\n",
        "                    tolerance=simplify_tolerance,\n",
        "                    preserve_topology=True\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"WARNING: Geometry simplification failed: {e}\")\n",
        "\n",
        "        # Store GeoJSON\n",
        "        geojson_results[timestamp] = geodata.__geo_interface__\n",
        "\n",
        "    print(f\"Processed {len(geojson_results)} timestamps successfully\")\n",
        "    return geojson_results"
      ],
      "metadata": {
        "id": "ZORSgcRm5owK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geojson_results = process_era5_grid_optimized(\n",
        "    era5_data=df_grouped,\n",
        "    time_column='valid_time',\n",
        "    lat_column='lat_rounded',\n",
        "    lon_column='lon_rounded',\n",
        "    value_column='msl',\n",
        "    grid_cell_size=1.0,\n",
        "    round_decimals=1,\n",
        "    simplify_tolerance = 0.01,\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "geojson_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuxFtINQ5r3f",
        "outputId": "4e7c4fae-3a8e-49df-e6a6-7e9cc272426c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data for 2020-05-01 00:00:00...\n",
            "Processing data for 2020-05-01 06:00:00...\n",
            "Processing data for 2020-05-01 12:00:00...\n",
            "Processing data for 2020-05-01 18:00:00...\n",
            "Processing data for 2020-05-02 00:00:00...\n",
            "Processing data for 2020-05-02 06:00:00...\n",
            "Processing data for 2020-05-02 12:00:00...\n",
            "Processing data for 2020-05-02 18:00:00...\n",
            "Processing data for 2020-05-03 00:00:00...\n",
            "Processing data for 2020-05-03 06:00:00...\n",
            "Processing data for 2020-05-03 12:00:00...\n",
            "Processing data for 2020-05-03 18:00:00...\n",
            "Processing data for 2020-05-04 00:00:00...\n",
            "Processing data for 2020-05-04 06:00:00...\n",
            "Processing data for 2020-05-04 12:00:00...\n",
            "Processing data for 2020-05-04 18:00:00...\n",
            "Processing data for 2020-05-05 00:00:00...\n",
            "Processing data for 2020-05-05 06:00:00...\n",
            "Processing data for 2020-05-05 12:00:00...\n",
            "Processing data for 2020-05-05 18:00:00...\n",
            "Processing data for 2020-05-06 00:00:00...\n",
            "Processing data for 2020-05-06 06:00:00...\n",
            "Processing data for 2020-05-06 12:00:00...\n",
            "Processing data for 2020-05-06 18:00:00...\n",
            "Processing data for 2020-05-07 00:00:00...\n",
            "Processing data for 2020-05-07 06:00:00...\n",
            "Processing data for 2020-05-07 12:00:00...\n",
            "Processing data for 2020-05-07 18:00:00...\n",
            "Processing data for 2020-05-08 00:00:00...\n",
            "Processing data for 2020-05-08 06:00:00...\n",
            "Processing data for 2020-05-08 12:00:00...\n",
            "Processing data for 2020-05-08 18:00:00...\n",
            "Processing data for 2020-05-09 00:00:00...\n",
            "Processing data for 2020-05-09 06:00:00...\n",
            "Processing data for 2020-05-09 12:00:00...\n",
            "Processing data for 2020-05-09 18:00:00...\n",
            "Processing data for 2020-05-10 00:00:00...\n",
            "Processing data for 2020-05-10 06:00:00...\n",
            "Processing data for 2020-05-10 12:00:00...\n",
            "Processing data for 2020-05-10 18:00:00...\n",
            "Processing data for 2020-05-11 00:00:00...\n",
            "Processing data for 2020-05-11 06:00:00...\n",
            "Processing data for 2020-05-11 12:00:00...\n",
            "Processing data for 2020-05-11 18:00:00...\n",
            "Processing data for 2020-05-12 00:00:00...\n",
            "Processing data for 2020-05-12 06:00:00...\n",
            "Processing data for 2020-05-12 12:00:00...\n",
            "Processing data for 2020-05-12 18:00:00...\n",
            "Processing data for 2020-05-13 00:00:00...\n",
            "Processing data for 2020-05-13 06:00:00...\n",
            "Processing data for 2020-05-13 12:00:00...\n",
            "Processing data for 2020-05-13 18:00:00...\n",
            "Processing data for 2020-05-14 00:00:00...\n",
            "Processing data for 2020-05-14 06:00:00...\n",
            "Processing data for 2020-05-14 12:00:00...\n",
            "Processing data for 2020-05-14 18:00:00...\n",
            "Processing data for 2020-05-15 00:00:00...\n",
            "Processing data for 2020-05-15 06:00:00...\n",
            "Processing data for 2020-05-15 12:00:00...\n",
            "Processing data for 2020-05-15 18:00:00...\n",
            "Processing data for 2020-05-16 00:00:00...\n",
            "Processing data for 2020-05-16 06:00:00...\n",
            "Processing data for 2020-05-16 12:00:00...\n",
            "Processing data for 2020-05-16 18:00:00...\n",
            "Processing data for 2020-05-17 00:00:00...\n",
            "Processing data for 2020-05-17 06:00:00...\n",
            "Processing data for 2020-05-17 12:00:00...\n",
            "Processing data for 2020-05-17 18:00:00...\n",
            "Processing data for 2020-05-18 00:00:00...\n",
            "Processing data for 2020-05-18 06:00:00...\n",
            "Processing data for 2020-05-18 12:00:00...\n",
            "Processing data for 2020-05-18 18:00:00...\n",
            "Processing data for 2020-05-19 00:00:00...\n",
            "Processing data for 2020-05-19 06:00:00...\n",
            "Processing data for 2020-05-19 12:00:00...\n",
            "Processing data for 2020-05-19 18:00:00...\n",
            "Processing data for 2020-05-20 00:00:00...\n",
            "Processing data for 2020-05-20 06:00:00...\n",
            "Processing data for 2020-05-20 12:00:00...\n",
            "Processing data for 2020-05-20 18:00:00...\n",
            "Processing data for 2020-05-21 00:00:00...\n",
            "Processing data for 2020-05-21 06:00:00...\n",
            "Processing data for 2020-05-21 12:00:00...\n",
            "Processing data for 2020-05-21 18:00:00...\n",
            "Processing data for 2020-05-22 00:00:00...\n",
            "Processing data for 2020-05-22 06:00:00...\n",
            "Processing data for 2020-05-22 12:00:00...\n",
            "Processing data for 2020-05-22 18:00:00...\n",
            "Processing data for 2020-05-23 00:00:00...\n",
            "Processing data for 2020-05-23 06:00:00...\n",
            "Processing data for 2020-05-23 12:00:00...\n",
            "Processing data for 2020-05-23 18:00:00...\n",
            "Processing data for 2020-05-24 00:00:00...\n",
            "Processing data for 2020-05-24 06:00:00...\n",
            "Processing data for 2020-05-24 12:00:00...\n",
            "Processing data for 2020-05-24 18:00:00...\n",
            "Processing data for 2020-05-25 00:00:00...\n",
            "Processing data for 2020-05-25 06:00:00...\n",
            "Processing data for 2020-05-25 12:00:00...\n",
            "Processing data for 2020-05-25 18:00:00...\n",
            "Processing data for 2020-05-26 00:00:00...\n",
            "Processing data for 2020-05-26 06:00:00...\n",
            "Processing data for 2020-05-26 12:00:00...\n",
            "Processing data for 2020-05-26 18:00:00...\n",
            "Processing data for 2020-05-27 00:00:00...\n",
            "Processing data for 2020-05-27 06:00:00...\n",
            "Processing data for 2020-05-27 12:00:00...\n",
            "Processing data for 2020-05-27 18:00:00...\n",
            "Processing data for 2020-05-28 00:00:00...\n",
            "Processing data for 2020-05-28 06:00:00...\n",
            "Processing data for 2020-05-28 12:00:00...\n",
            "Processing data for 2020-05-28 18:00:00...\n",
            "Processing data for 2020-05-29 00:00:00...\n",
            "Processing data for 2020-05-29 06:00:00...\n",
            "Processing data for 2020-05-29 12:00:00...\n",
            "Processing data for 2020-05-29 18:00:00...\n",
            "Processing data for 2020-05-30 00:00:00...\n",
            "Processing data for 2020-05-30 06:00:00...\n",
            "Processing data for 2020-05-30 12:00:00...\n",
            "Processing data for 2020-05-30 18:00:00...\n",
            "Processing data for 2020-05-31 00:00:00...\n",
            "Processing data for 2020-05-31 06:00:00...\n",
            "Processing data for 2020-05-31 12:00:00...\n",
            "Processing data for 2020-05-31 18:00:00...\n",
            "Processing data for 2020-06-01 00:00:00...\n",
            "Processing data for 2020-06-01 06:00:00...\n",
            "Processing data for 2020-06-01 12:00:00...\n",
            "Processing data for 2020-06-01 18:00:00...\n",
            "Processing data for 2020-06-02 00:00:00...\n",
            "Processing data for 2020-06-02 06:00:00...\n",
            "Processing data for 2020-06-02 12:00:00...\n",
            "Processing data for 2020-06-02 18:00:00...\n",
            "Processing data for 2020-06-03 00:00:00...\n",
            "Processing data for 2020-06-03 06:00:00...\n",
            "Processing data for 2020-06-03 12:00:00...\n",
            "Processing data for 2020-06-03 18:00:00...\n",
            "Processing data for 2020-06-04 00:00:00...\n",
            "Processing data for 2020-06-04 06:00:00...\n",
            "Processing data for 2020-06-04 12:00:00...\n",
            "Processing data for 2020-06-04 18:00:00...\n",
            "Processing data for 2020-06-05 00:00:00...\n",
            "Processing data for 2020-06-05 06:00:00...\n",
            "Processing data for 2020-06-05 12:00:00...\n",
            "Processing data for 2020-06-05 18:00:00...\n",
            "Processing data for 2020-06-06 00:00:00...\n",
            "Processing data for 2020-06-06 06:00:00...\n",
            "Processing data for 2020-06-06 12:00:00...\n",
            "Processing data for 2020-06-06 18:00:00...\n",
            "Processing data for 2020-06-07 00:00:00...\n",
            "Processing data for 2020-06-07 06:00:00...\n",
            "Processing data for 2020-06-07 12:00:00...\n",
            "Processing data for 2020-06-07 18:00:00...\n",
            "Processing data for 2020-06-08 00:00:00...\n",
            "Processing data for 2020-06-08 06:00:00...\n",
            "Processing data for 2020-06-08 12:00:00...\n",
            "Processing data for 2020-06-08 18:00:00...\n",
            "Processing data for 2020-06-09 00:00:00...\n",
            "Processing data for 2020-06-09 06:00:00...\n",
            "Processing data for 2020-06-09 12:00:00...\n",
            "Processing data for 2020-06-09 18:00:00...\n",
            "Processing data for 2020-06-10 00:00:00...\n",
            "Processing data for 2020-06-10 06:00:00...\n",
            "Processing data for 2020-06-10 12:00:00...\n",
            "Processing data for 2020-06-10 18:00:00...\n",
            "Processing data for 2020-06-11 00:00:00...\n",
            "Processing data for 2020-06-11 06:00:00...\n",
            "Processing data for 2020-06-11 12:00:00...\n",
            "Processing data for 2020-06-11 18:00:00...\n",
            "Processing data for 2020-06-12 00:00:00...\n",
            "Processing data for 2020-06-12 06:00:00...\n",
            "Processing data for 2020-06-12 12:00:00...\n",
            "Processing data for 2020-06-12 18:00:00...\n",
            "Processing data for 2020-06-13 00:00:00...\n",
            "Processing data for 2020-06-13 06:00:00...\n",
            "Processing data for 2020-06-13 12:00:00...\n",
            "Processing data for 2020-06-13 18:00:00...\n",
            "Processing data for 2020-06-14 00:00:00...\n",
            "Processing data for 2020-06-14 06:00:00...\n",
            "Processing data for 2020-06-14 12:00:00...\n",
            "Processing data for 2020-06-14 18:00:00...\n",
            "Processing data for 2020-06-15 00:00:00...\n",
            "Processing data for 2020-06-15 06:00:00...\n",
            "Processing data for 2020-06-15 12:00:00...\n",
            "Processing data for 2020-06-15 18:00:00...\n",
            "Processing data for 2020-06-16 00:00:00...\n",
            "Processing data for 2020-06-16 06:00:00...\n",
            "Processing data for 2020-06-16 12:00:00...\n",
            "Processing data for 2020-06-16 18:00:00...\n",
            "Processing data for 2020-06-17 00:00:00...\n",
            "Processing data for 2020-06-17 06:00:00...\n",
            "Processing data for 2020-06-17 12:00:00...\n",
            "Processing data for 2020-06-17 18:00:00...\n",
            "Processing data for 2020-06-18 00:00:00...\n",
            "Processing data for 2020-06-18 06:00:00...\n",
            "Processing data for 2020-06-18 12:00:00...\n",
            "Processing data for 2020-06-18 18:00:00...\n",
            "Processing data for 2020-06-19 00:00:00...\n",
            "Processing data for 2020-06-19 06:00:00...\n",
            "Processing data for 2020-06-19 12:00:00...\n",
            "Processing data for 2020-06-19 18:00:00...\n",
            "Processing data for 2020-06-20 00:00:00...\n",
            "Processing data for 2020-06-20 06:00:00...\n",
            "Processing data for 2020-06-20 12:00:00...\n",
            "Processing data for 2020-06-20 18:00:00...\n",
            "Processing data for 2020-06-21 00:00:00...\n",
            "Processing data for 2020-06-21 06:00:00...\n",
            "Processing data for 2020-06-21 12:00:00...\n",
            "Processing data for 2020-06-21 18:00:00...\n",
            "Processing data for 2020-06-22 00:00:00...\n",
            "Processing data for 2020-06-22 06:00:00...\n",
            "Processing data for 2020-06-22 12:00:00...\n",
            "Processing data for 2020-06-22 18:00:00...\n",
            "Processing data for 2020-06-23 00:00:00...\n",
            "Processing data for 2020-06-23 06:00:00...\n",
            "Processing data for 2020-06-23 12:00:00...\n",
            "Processing data for 2020-06-23 18:00:00...\n",
            "Processing data for 2020-06-24 00:00:00...\n",
            "Processing data for 2020-06-24 06:00:00...\n",
            "Processing data for 2020-06-24 12:00:00...\n",
            "Processing data for 2020-06-24 18:00:00...\n",
            "Processing data for 2020-06-25 00:00:00...\n",
            "Processing data for 2020-06-25 06:00:00...\n",
            "Processing data for 2020-06-25 12:00:00...\n",
            "Processing data for 2020-06-25 18:00:00...\n",
            "Processing data for 2020-06-26 00:00:00...\n",
            "Processing data for 2020-06-26 06:00:00...\n",
            "Processing data for 2020-06-26 12:00:00...\n",
            "Processing data for 2020-06-26 18:00:00...\n",
            "Processing data for 2020-06-27 00:00:00...\n",
            "Processing data for 2020-06-27 06:00:00...\n",
            "Processing data for 2020-06-27 12:00:00...\n",
            "Processing data for 2020-06-27 18:00:00...\n",
            "Processing data for 2020-06-28 00:00:00...\n",
            "Processing data for 2020-06-28 06:00:00...\n",
            "Processing data for 2020-06-28 12:00:00...\n",
            "Processing data for 2020-06-28 18:00:00...\n",
            "Processing data for 2020-06-29 00:00:00...\n",
            "Processing data for 2020-06-29 06:00:00...\n",
            "Processing data for 2020-06-29 12:00:00...\n",
            "Processing data for 2020-06-29 18:00:00...\n",
            "Processing data for 2020-06-30 00:00:00...\n",
            "Processing data for 2020-06-30 06:00:00...\n",
            "Processing data for 2020-06-30 12:00:00...\n",
            "Processing data for 2020-06-30 18:00:00...\n",
            "Processing data for 2020-07-01 00:00:00...\n",
            "Processing data for 2020-07-01 06:00:00...\n",
            "Processing data for 2020-07-01 12:00:00...\n",
            "Processing data for 2020-07-01 18:00:00...\n",
            "Processing data for 2020-07-02 00:00:00...\n",
            "Processing data for 2020-07-02 06:00:00...\n",
            "Processing data for 2020-07-02 12:00:00...\n",
            "Processing data for 2020-07-02 18:00:00...\n",
            "Processing data for 2020-07-03 00:00:00...\n",
            "Processing data for 2020-07-03 06:00:00...\n",
            "Processing data for 2020-07-03 12:00:00...\n",
            "Processing data for 2020-07-03 18:00:00...\n",
            "Processing data for 2020-07-04 00:00:00...\n",
            "Processing data for 2020-07-04 06:00:00...\n",
            "Processing data for 2020-07-04 12:00:00...\n",
            "Processing data for 2020-07-04 18:00:00...\n",
            "Processing data for 2020-07-05 00:00:00...\n",
            "Processing data for 2020-07-05 06:00:00...\n",
            "Processing data for 2020-07-05 12:00:00...\n",
            "Processing data for 2020-07-05 18:00:00...\n",
            "Processing data for 2020-07-06 00:00:00...\n",
            "Processing data for 2020-07-06 06:00:00...\n",
            "Processing data for 2020-07-06 12:00:00...\n",
            "Processing data for 2020-07-06 18:00:00...\n",
            "Processing data for 2020-07-07 00:00:00...\n",
            "Processing data for 2020-07-07 06:00:00...\n",
            "Processing data for 2020-07-07 12:00:00...\n",
            "Processing data for 2020-07-07 18:00:00...\n",
            "Processing data for 2020-07-08 00:00:00...\n",
            "Processing data for 2020-07-08 06:00:00...\n",
            "Processing data for 2020-07-08 12:00:00...\n",
            "Processing data for 2020-07-08 18:00:00...\n",
            "Processing data for 2020-07-09 00:00:00...\n",
            "Processing data for 2020-07-09 06:00:00...\n",
            "Processing data for 2020-07-09 12:00:00...\n",
            "Processing data for 2020-07-09 18:00:00...\n",
            "Processing data for 2020-07-10 00:00:00...\n",
            "Processing data for 2020-07-10 06:00:00...\n",
            "Processing data for 2020-07-10 12:00:00...\n",
            "Processing data for 2020-07-10 18:00:00...\n",
            "Processing data for 2020-07-11 00:00:00...\n",
            "Processing data for 2020-07-11 06:00:00...\n",
            "Processing data for 2020-07-11 12:00:00...\n",
            "Processing data for 2020-07-11 18:00:00...\n",
            "Processing data for 2020-07-12 00:00:00...\n",
            "Processing data for 2020-07-12 06:00:00...\n",
            "Processing data for 2020-07-12 12:00:00...\n",
            "Processing data for 2020-07-12 18:00:00...\n",
            "Processing data for 2020-07-13 00:00:00...\n",
            "Processing data for 2020-07-13 06:00:00...\n",
            "Processing data for 2020-07-13 12:00:00...\n",
            "Processing data for 2020-07-13 18:00:00...\n",
            "Processing data for 2020-07-14 00:00:00...\n",
            "Processing data for 2020-07-14 06:00:00...\n",
            "Processing data for 2020-07-14 12:00:00...\n",
            "Processing data for 2020-07-14 18:00:00...\n",
            "Processing data for 2020-07-15 00:00:00...\n",
            "Processing data for 2020-07-15 06:00:00...\n",
            "Processing data for 2020-07-15 12:00:00...\n",
            "Processing data for 2020-07-15 18:00:00...\n",
            "Processing data for 2020-07-16 00:00:00...\n",
            "Processing data for 2020-07-16 06:00:00...\n",
            "Processing data for 2020-07-16 12:00:00...\n",
            "Processing data for 2020-07-16 18:00:00...\n",
            "Processing data for 2020-07-17 00:00:00...\n",
            "Processing data for 2020-07-17 06:00:00...\n",
            "Processing data for 2020-07-17 12:00:00...\n",
            "Processing data for 2020-07-17 18:00:00...\n",
            "Processing data for 2020-07-18 00:00:00...\n",
            "Processing data for 2020-07-18 06:00:00...\n",
            "Processing data for 2020-07-18 12:00:00...\n",
            "Processing data for 2020-07-18 18:00:00...\n",
            "Processing data for 2020-07-19 00:00:00...\n",
            "Processing data for 2020-07-19 06:00:00...\n",
            "Processing data for 2020-07-19 12:00:00...\n",
            "Processing data for 2020-07-19 18:00:00...\n",
            "Processing data for 2020-07-20 00:00:00...\n",
            "Processing data for 2020-07-20 06:00:00...\n",
            "Processing data for 2020-07-20 12:00:00...\n",
            "Processing data for 2020-07-20 18:00:00...\n",
            "Processing data for 2020-07-21 00:00:00...\n",
            "Processing data for 2020-07-21 06:00:00...\n",
            "Processing data for 2020-07-21 12:00:00...\n",
            "Processing data for 2020-07-21 18:00:00...\n",
            "Processing data for 2020-07-22 00:00:00...\n",
            "Processing data for 2020-07-22 06:00:00...\n",
            "Processing data for 2020-07-22 12:00:00...\n",
            "Processing data for 2020-07-22 18:00:00...\n",
            "Processing data for 2020-07-23 00:00:00...\n",
            "Processing data for 2020-07-23 06:00:00...\n",
            "Processing data for 2020-07-23 12:00:00...\n",
            "Processing data for 2020-07-23 18:00:00...\n",
            "Processing data for 2020-07-24 00:00:00...\n",
            "Processing data for 2020-07-24 06:00:00...\n",
            "Processing data for 2020-07-24 12:00:00...\n",
            "Processing data for 2020-07-24 18:00:00...\n",
            "Processing data for 2020-07-25 00:00:00...\n",
            "Processing data for 2020-07-25 06:00:00...\n",
            "Processing data for 2020-07-25 12:00:00...\n",
            "Processing data for 2020-07-25 18:00:00...\n",
            "Processing data for 2020-07-26 00:00:00...\n",
            "Processing data for 2020-07-26 06:00:00...\n",
            "Processing data for 2020-07-26 12:00:00...\n",
            "Processing data for 2020-07-26 18:00:00...\n",
            "Processing data for 2020-07-27 00:00:00...\n",
            "Processing data for 2020-07-27 06:00:00...\n",
            "Processing data for 2020-07-27 12:00:00...\n",
            "Processing data for 2020-07-27 18:00:00...\n",
            "Processing data for 2020-07-28 00:00:00...\n",
            "Processing data for 2020-07-28 06:00:00...\n",
            "Processing data for 2020-07-28 12:00:00...\n",
            "Processing data for 2020-07-28 18:00:00...\n",
            "Processing data for 2020-07-29 00:00:00...\n",
            "Processing data for 2020-07-29 06:00:00...\n",
            "Processing data for 2020-07-29 12:00:00...\n",
            "Processing data for 2020-07-29 18:00:00...\n",
            "Processing data for 2020-07-30 00:00:00...\n",
            "Processing data for 2020-07-30 06:00:00...\n",
            "Processing data for 2020-07-30 12:00:00...\n",
            "Processing data for 2020-07-30 18:00:00...\n",
            "Processing data for 2020-07-31 00:00:00...\n",
            "Processing data for 2020-07-31 06:00:00...\n",
            "Processing data for 2020-07-31 12:00:00...\n",
            "Processing data for 2020-07-31 18:00:00...\n",
            "Processing data for 2020-08-01 00:00:00...\n",
            "Processing data for 2020-08-01 06:00:00...\n",
            "Processing data for 2020-08-01 12:00:00...\n",
            "Processing data for 2020-08-01 18:00:00...\n",
            "Processing data for 2020-08-02 00:00:00...\n",
            "Processing data for 2020-08-02 06:00:00...\n",
            "Processing data for 2020-08-02 12:00:00...\n",
            "Processing data for 2020-08-02 18:00:00...\n",
            "Processing data for 2020-08-03 00:00:00...\n",
            "Processing data for 2020-08-03 06:00:00...\n",
            "Processing data for 2020-08-03 12:00:00...\n",
            "Processing data for 2020-08-03 18:00:00...\n",
            "Processing data for 2020-08-04 00:00:00...\n",
            "Processing data for 2020-08-04 06:00:00...\n",
            "Processing data for 2020-08-04 12:00:00...\n",
            "Processing data for 2020-08-04 18:00:00...\n",
            "Processing data for 2020-08-05 00:00:00...\n",
            "Processing data for 2020-08-05 06:00:00...\n",
            "Processing data for 2020-08-05 12:00:00...\n",
            "Processing data for 2020-08-05 18:00:00...\n",
            "Processing data for 2020-08-06 00:00:00...\n",
            "Processing data for 2020-08-06 06:00:00...\n",
            "Processing data for 2020-08-06 12:00:00...\n",
            "Processing data for 2020-08-06 18:00:00...\n",
            "Processing data for 2020-08-07 00:00:00...\n",
            "Processing data for 2020-08-07 06:00:00...\n",
            "Processing data for 2020-08-07 12:00:00...\n",
            "Processing data for 2020-08-07 18:00:00...\n",
            "Processing data for 2020-08-08 00:00:00...\n",
            "Processing data for 2020-08-08 06:00:00...\n",
            "Processing data for 2020-08-08 12:00:00...\n",
            "Processing data for 2020-08-08 18:00:00...\n",
            "Processing data for 2020-08-09 00:00:00...\n",
            "Processing data for 2020-08-09 06:00:00...\n",
            "Processing data for 2020-08-09 12:00:00...\n",
            "Processing data for 2020-08-09 18:00:00...\n",
            "Processing data for 2020-08-10 00:00:00...\n",
            "Processing data for 2020-08-10 06:00:00...\n",
            "Processing data for 2020-08-10 12:00:00...\n",
            "Processing data for 2020-08-10 18:00:00...\n",
            "Processing data for 2020-08-11 00:00:00...\n",
            "Processing data for 2020-08-11 06:00:00...\n",
            "Processing data for 2020-08-11 12:00:00...\n",
            "Processing data for 2020-08-11 18:00:00...\n",
            "Processing data for 2020-08-12 00:00:00...\n",
            "Processing data for 2020-08-12 06:00:00...\n",
            "Processing data for 2020-08-12 12:00:00...\n",
            "Processing data for 2020-08-12 18:00:00...\n",
            "Processing data for 2020-08-13 00:00:00...\n",
            "Processing data for 2020-08-13 06:00:00...\n",
            "Processing data for 2020-08-13 12:00:00...\n",
            "Processing data for 2020-08-13 18:00:00...\n",
            "Processing data for 2020-08-14 00:00:00...\n",
            "Processing data for 2020-08-14 06:00:00...\n",
            "Processing data for 2020-08-14 12:00:00...\n",
            "Processing data for 2020-08-14 18:00:00...\n",
            "Processing data for 2020-08-15 00:00:00...\n",
            "Processing data for 2020-08-15 06:00:00...\n",
            "Processing data for 2020-08-15 12:00:00...\n",
            "Processing data for 2020-08-15 18:00:00...\n",
            "Processing data for 2020-08-16 00:00:00...\n",
            "Processing data for 2020-08-16 06:00:00...\n",
            "Processing data for 2020-08-16 12:00:00...\n",
            "Processing data for 2020-08-16 18:00:00...\n",
            "Processing data for 2020-08-17 00:00:00...\n",
            "Processing data for 2020-08-17 06:00:00...\n",
            "Processing data for 2020-08-17 12:00:00...\n",
            "Processing data for 2020-08-17 18:00:00...\n",
            "Processing data for 2020-08-18 00:00:00...\n",
            "Processing data for 2020-08-18 06:00:00...\n",
            "Processing data for 2020-08-18 12:00:00...\n",
            "Processing data for 2020-08-18 18:00:00...\n",
            "Processing data for 2020-08-19 00:00:00...\n",
            "Processing data for 2020-08-19 06:00:00...\n",
            "Processing data for 2020-08-19 12:00:00...\n",
            "Processing data for 2020-08-19 18:00:00...\n",
            "Processing data for 2020-08-20 00:00:00...\n",
            "Processing data for 2020-08-20 06:00:00...\n",
            "Processing data for 2020-08-20 12:00:00...\n",
            "Processing data for 2020-08-20 18:00:00...\n",
            "Processing data for 2020-08-21 00:00:00...\n",
            "Processing data for 2020-08-21 06:00:00...\n",
            "Processing data for 2020-08-21 12:00:00...\n",
            "Processing data for 2020-08-21 18:00:00...\n",
            "Processing data for 2020-08-22 00:00:00...\n",
            "Processing data for 2020-08-22 06:00:00...\n",
            "Processing data for 2020-08-22 12:00:00...\n",
            "Processing data for 2020-08-22 18:00:00...\n",
            "Processing data for 2020-08-23 00:00:00...\n",
            "Processing data for 2020-08-23 06:00:00...\n",
            "Processing data for 2020-08-23 12:00:00...\n",
            "Processing data for 2020-08-23 18:00:00...\n",
            "Processing data for 2020-08-24 00:00:00...\n",
            "Processing data for 2020-08-24 06:00:00...\n",
            "Processing data for 2020-08-24 12:00:00...\n",
            "Processing data for 2020-08-24 18:00:00...\n",
            "Processing data for 2020-08-25 00:00:00...\n",
            "Processing data for 2020-08-25 06:00:00...\n",
            "Processing data for 2020-08-25 12:00:00...\n",
            "Processing data for 2020-08-25 18:00:00...\n",
            "Processing data for 2020-08-26 00:00:00...\n",
            "Processing data for 2020-08-26 06:00:00...\n",
            "Processing data for 2020-08-26 12:00:00...\n",
            "Processing data for 2020-08-26 18:00:00...\n",
            "Processing data for 2020-08-27 00:00:00...\n",
            "Processing data for 2020-08-27 06:00:00...\n",
            "Processing data for 2020-08-27 12:00:00...\n",
            "Processing data for 2020-08-27 18:00:00...\n",
            "Processing data for 2020-08-28 00:00:00...\n",
            "Processing data for 2020-08-28 06:00:00...\n",
            "Processing data for 2020-08-28 12:00:00...\n",
            "Processing data for 2020-08-28 18:00:00...\n",
            "Processing data for 2020-08-29 00:00:00...\n",
            "Processing data for 2020-08-29 06:00:00...\n",
            "Processing data for 2020-08-29 12:00:00...\n",
            "Processing data for 2020-08-29 18:00:00...\n",
            "Processing data for 2020-08-30 00:00:00...\n",
            "Processing data for 2020-08-30 06:00:00...\n",
            "Processing data for 2020-08-30 12:00:00...\n",
            "Processing data for 2020-08-30 18:00:00...\n",
            "Processing data for 2020-08-31 00:00:00...\n",
            "Processing data for 2020-08-31 06:00:00...\n",
            "Processing data for 2020-08-31 12:00:00...\n",
            "Processing data for 2020-08-31 18:00:00...\n",
            "Processing data for 2020-09-01 00:00:00...\n",
            "Processing data for 2020-09-01 06:00:00...\n",
            "Processing data for 2020-09-01 12:00:00...\n",
            "Processing data for 2020-09-01 18:00:00...\n",
            "Processing data for 2020-09-02 00:00:00...\n",
            "Processing data for 2020-09-02 06:00:00...\n",
            "Processing data for 2020-09-02 12:00:00...\n",
            "Processing data for 2020-09-02 18:00:00...\n",
            "Processing data for 2020-09-03 00:00:00...\n",
            "Processing data for 2020-09-03 06:00:00...\n",
            "Processing data for 2020-09-03 12:00:00...\n",
            "Processing data for 2020-09-03 18:00:00...\n",
            "Processing data for 2020-09-04 00:00:00...\n",
            "Processing data for 2020-09-04 06:00:00...\n",
            "Processing data for 2020-09-04 12:00:00...\n",
            "Processing data for 2020-09-04 18:00:00...\n",
            "Processing data for 2020-09-05 00:00:00...\n",
            "Processing data for 2020-09-05 06:00:00...\n",
            "Processing data for 2020-09-05 12:00:00...\n",
            "Processing data for 2020-09-05 18:00:00...\n",
            "Processing data for 2020-09-06 00:00:00...\n",
            "Processing data for 2020-09-06 06:00:00...\n",
            "Processing data for 2020-09-06 12:00:00...\n",
            "Processing data for 2020-09-06 18:00:00...\n",
            "Processing data for 2020-09-07 00:00:00...\n",
            "Processing data for 2020-09-07 06:00:00...\n",
            "Processing data for 2020-09-07 12:00:00...\n",
            "Processing data for 2020-09-07 18:00:00...\n",
            "Processing data for 2020-09-08 00:00:00...\n",
            "Processing data for 2020-09-08 06:00:00...\n",
            "Processing data for 2020-09-08 12:00:00...\n",
            "Processing data for 2020-09-08 18:00:00...\n",
            "Processing data for 2020-09-09 00:00:00...\n",
            "Processing data for 2020-09-09 06:00:00...\n",
            "Processing data for 2020-09-09 12:00:00...\n",
            "Processing data for 2020-09-09 18:00:00...\n",
            "Processing data for 2020-09-10 00:00:00...\n",
            "Processing data for 2020-09-10 06:00:00...\n",
            "Processing data for 2020-09-10 12:00:00...\n",
            "Processing data for 2020-09-10 18:00:00...\n",
            "Processing data for 2020-09-11 00:00:00...\n",
            "Processing data for 2020-09-11 06:00:00...\n",
            "Processing data for 2020-09-11 12:00:00...\n",
            "Processing data for 2020-09-11 18:00:00...\n",
            "Processing data for 2020-09-12 00:00:00...\n",
            "Processing data for 2020-09-12 06:00:00...\n",
            "Processing data for 2020-09-12 12:00:00...\n",
            "Processing data for 2020-09-12 18:00:00...\n",
            "Processing data for 2020-09-13 00:00:00...\n",
            "Processing data for 2020-09-13 06:00:00...\n",
            "Processing data for 2020-09-13 12:00:00...\n",
            "Processing data for 2020-09-13 18:00:00...\n",
            "Processing data for 2020-09-14 00:00:00...\n",
            "Processing data for 2020-09-14 06:00:00...\n",
            "Processing data for 2020-09-14 12:00:00...\n",
            "Processing data for 2020-09-14 18:00:00...\n",
            "Processing data for 2020-09-15 00:00:00...\n",
            "Processing data for 2020-09-15 06:00:00...\n",
            "Processing data for 2020-09-15 12:00:00...\n",
            "Processing data for 2020-09-15 18:00:00...\n",
            "Processing data for 2020-09-16 00:00:00...\n",
            "Processing data for 2020-09-16 06:00:00...\n",
            "Processing data for 2020-09-16 12:00:00...\n",
            "Processing data for 2020-09-16 18:00:00...\n",
            "Processing data for 2020-09-17 00:00:00...\n",
            "Processing data for 2020-09-17 06:00:00...\n",
            "Processing data for 2020-09-17 12:00:00...\n",
            "Processing data for 2020-09-17 18:00:00...\n",
            "Processing data for 2020-09-18 00:00:00...\n",
            "Processing data for 2020-09-18 06:00:00...\n",
            "Processing data for 2020-09-18 12:00:00...\n",
            "Processing data for 2020-09-18 18:00:00...\n",
            "Processing data for 2020-09-19 00:00:00...\n",
            "Processing data for 2020-09-19 06:00:00...\n",
            "Processing data for 2020-09-19 12:00:00...\n",
            "Processing data for 2020-09-19 18:00:00...\n",
            "Processing data for 2020-09-20 00:00:00...\n",
            "Processing data for 2020-09-20 06:00:00...\n",
            "Processing data for 2020-09-20 12:00:00...\n",
            "Processing data for 2020-09-20 18:00:00...\n",
            "Processing data for 2020-09-21 00:00:00...\n",
            "Processing data for 2020-09-21 06:00:00...\n",
            "Processing data for 2020-09-21 12:00:00...\n",
            "Processing data for 2020-09-21 18:00:00...\n",
            "Processing data for 2020-09-22 00:00:00...\n",
            "Processing data for 2020-09-22 06:00:00...\n",
            "Processing data for 2020-09-22 12:00:00...\n",
            "Processing data for 2020-09-22 18:00:00...\n",
            "Processing data for 2020-09-23 00:00:00...\n",
            "Processing data for 2020-09-23 06:00:00...\n",
            "Processing data for 2020-09-23 12:00:00...\n",
            "Processing data for 2020-09-23 18:00:00...\n",
            "Processing data for 2020-09-24 00:00:00...\n",
            "Processing data for 2020-09-24 06:00:00...\n",
            "Processing data for 2020-09-24 12:00:00...\n",
            "Processing data for 2020-09-24 18:00:00...\n",
            "Processing data for 2020-09-25 00:00:00...\n",
            "Processing data for 2020-09-25 06:00:00...\n",
            "Processing data for 2020-09-25 12:00:00...\n",
            "Processing data for 2020-09-25 18:00:00...\n",
            "Processing data for 2020-09-26 00:00:00...\n",
            "Processing data for 2020-09-26 06:00:00...\n",
            "Processing data for 2020-09-26 12:00:00...\n",
            "Processing data for 2020-09-26 18:00:00...\n",
            "Processing data for 2020-09-27 00:00:00...\n",
            "Processing data for 2020-09-27 06:00:00...\n",
            "Processing data for 2020-09-27 12:00:00...\n",
            "Processing data for 2020-09-27 18:00:00...\n",
            "Processing data for 2020-09-28 00:00:00...\n",
            "Processing data for 2020-09-28 06:00:00...\n",
            "Processing data for 2020-09-28 12:00:00...\n",
            "Processing data for 2020-09-28 18:00:00...\n",
            "Processing data for 2020-09-29 00:00:00...\n",
            "Processing data for 2020-09-29 06:00:00...\n",
            "Processing data for 2020-09-29 12:00:00...\n",
            "Processing data for 2020-09-29 18:00:00...\n",
            "Processing data for 2020-09-30 00:00:00...\n",
            "Processing data for 2020-09-30 06:00:00...\n",
            "Processing data for 2020-09-30 12:00:00...\n",
            "Processing data for 2020-09-30 18:00:00...\n",
            "Processing data for 2020-10-01 00:00:00...\n",
            "Processing data for 2020-10-01 06:00:00...\n",
            "Processing data for 2020-10-01 12:00:00...\n",
            "Processing data for 2020-10-01 18:00:00...\n",
            "Processing data for 2020-10-02 00:00:00...\n",
            "Processing data for 2020-10-02 06:00:00...\n",
            "Processing data for 2020-10-02 12:00:00...\n",
            "Processing data for 2020-10-02 18:00:00...\n",
            "Processing data for 2020-10-03 00:00:00...\n",
            "Processing data for 2020-10-03 06:00:00...\n",
            "Processing data for 2020-10-03 12:00:00...\n",
            "Processing data for 2020-10-03 18:00:00...\n",
            "Processing data for 2020-10-04 00:00:00...\n",
            "Processing data for 2020-10-04 06:00:00...\n",
            "Processing data for 2020-10-04 12:00:00...\n",
            "Processing data for 2020-10-04 18:00:00...\n",
            "Processing data for 2020-10-05 00:00:00...\n",
            "Processing data for 2020-10-05 06:00:00...\n",
            "Processing data for 2020-10-05 12:00:00...\n",
            "Processing data for 2020-10-05 18:00:00...\n",
            "Processing data for 2020-10-06 00:00:00...\n",
            "Processing data for 2020-10-06 06:00:00...\n",
            "Processing data for 2020-10-06 12:00:00...\n",
            "Processing data for 2020-10-06 18:00:00...\n",
            "Processing data for 2020-10-07 00:00:00...\n",
            "Processing data for 2020-10-07 06:00:00...\n",
            "Processing data for 2020-10-07 12:00:00...\n",
            "Processing data for 2020-10-07 18:00:00...\n",
            "Processing data for 2020-10-08 00:00:00...\n",
            "Processing data for 2020-10-08 06:00:00...\n",
            "Processing data for 2020-10-08 12:00:00...\n",
            "Processing data for 2020-10-08 18:00:00...\n",
            "Processing data for 2020-10-09 00:00:00...\n",
            "Processing data for 2020-10-09 06:00:00...\n",
            "Processing data for 2020-10-09 12:00:00...\n",
            "Processing data for 2020-10-09 18:00:00...\n",
            "Processing data for 2020-10-10 00:00:00...\n",
            "Processing data for 2020-10-10 06:00:00...\n",
            "Processing data for 2020-10-10 12:00:00...\n",
            "Processing data for 2020-10-10 18:00:00...\n",
            "Processing data for 2020-10-11 00:00:00...\n",
            "Processing data for 2020-10-11 06:00:00...\n",
            "Processing data for 2020-10-11 12:00:00...\n",
            "Processing data for 2020-10-11 18:00:00...\n",
            "Processing data for 2020-10-12 00:00:00...\n",
            "Processing data for 2020-10-12 06:00:00...\n",
            "Processing data for 2020-10-12 12:00:00...\n",
            "Processing data for 2020-10-12 18:00:00...\n",
            "Processing data for 2020-10-13 00:00:00...\n",
            "Processing data for 2020-10-13 06:00:00...\n",
            "Processing data for 2020-10-13 12:00:00...\n",
            "Processing data for 2020-10-13 18:00:00...\n",
            "Processing data for 2020-10-14 00:00:00...\n",
            "Processing data for 2020-10-14 06:00:00...\n",
            "Processing data for 2020-10-14 12:00:00...\n",
            "Processing data for 2020-10-14 18:00:00...\n",
            "Processing data for 2020-10-15 00:00:00...\n",
            "Processing data for 2020-10-15 06:00:00...\n",
            "Processing data for 2020-10-15 12:00:00...\n",
            "Processing data for 2020-10-15 18:00:00...\n",
            "Processing data for 2020-10-16 00:00:00...\n",
            "Processing data for 2020-10-16 06:00:00...\n",
            "Processing data for 2020-10-16 12:00:00...\n",
            "Processing data for 2020-10-16 18:00:00...\n",
            "Processing data for 2020-10-17 00:00:00...\n",
            "Processing data for 2020-10-17 06:00:00...\n",
            "Processing data for 2020-10-17 12:00:00...\n",
            "Processing data for 2020-10-17 18:00:00...\n",
            "Processing data for 2020-10-18 00:00:00...\n",
            "Processing data for 2020-10-18 06:00:00...\n",
            "Processing data for 2020-10-18 12:00:00...\n",
            "Processing data for 2020-10-18 18:00:00...\n",
            "Processing data for 2020-10-19 00:00:00...\n",
            "Processing data for 2020-10-19 06:00:00...\n",
            "Processing data for 2020-10-19 12:00:00...\n",
            "Processing data for 2020-10-19 18:00:00...\n",
            "Processing data for 2020-10-20 00:00:00...\n",
            "Processing data for 2020-10-20 06:00:00...\n",
            "Processing data for 2020-10-20 12:00:00...\n",
            "Processing data for 2020-10-20 18:00:00...\n",
            "Processing data for 2020-10-21 00:00:00...\n",
            "Processing data for 2020-10-21 06:00:00...\n",
            "Processing data for 2020-10-21 12:00:00...\n",
            "Processing data for 2020-10-21 18:00:00...\n",
            "Processing data for 2020-10-22 00:00:00...\n",
            "Processing data for 2020-10-22 06:00:00...\n",
            "Processing data for 2020-10-22 12:00:00...\n",
            "Processing data for 2020-10-22 18:00:00...\n",
            "Processing data for 2020-10-23 00:00:00...\n",
            "Processing data for 2020-10-23 06:00:00...\n",
            "Processing data for 2020-10-23 12:00:00...\n",
            "Processing data for 2020-10-23 18:00:00...\n",
            "Processing data for 2020-10-24 00:00:00...\n",
            "Processing data for 2020-10-24 06:00:00...\n",
            "Processing data for 2020-10-24 12:00:00...\n",
            "Processing data for 2020-10-24 18:00:00...\n",
            "Processing data for 2020-10-25 00:00:00...\n",
            "Processing data for 2020-10-25 06:00:00...\n",
            "Processing data for 2020-10-25 12:00:00...\n",
            "Processing data for 2020-10-25 18:00:00...\n",
            "Processing data for 2020-10-26 00:00:00...\n",
            "Processing data for 2020-10-26 06:00:00...\n",
            "Processing data for 2020-10-26 12:00:00...\n",
            "Processing data for 2020-10-26 18:00:00...\n",
            "Processing data for 2020-10-27 00:00:00...\n",
            "Processing data for 2020-10-27 06:00:00...\n",
            "Processing data for 2020-10-27 12:00:00...\n",
            "Processing data for 2020-10-27 18:00:00...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "geojson_output_file='msl-2020-processed.geojson'\n",
        "with open(geojson_output_file, 'w') as f:\n",
        "        json.dump(geojson_results, f, indent=2)  # Use indent for readability\n",
        "\n",
        "print(f\"GeoJSON data written to: {geojson_output_file}\")"
      ],
      "metadata": {
        "id": "NO2KhlEe8IF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading geojson files to merge polygons and reduce file size"
      ],
      "metadata": {
        "id": "bQj0Dfx9bgon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from shapely.geometry import shape, mapping\n",
        "from shapely.ops import unary_union\n",
        "import numpy as np\n",
        "\n",
        "def kelvin_to_celsius(kelvin):\n",
        "    return float(kelvin) - 273.15\n",
        "\n",
        "def get_temp_bucket(temp_celsius):\n",
        "    \"\"\"\n",
        "    Assigns temperature to a fixed 0.5°C bucket\n",
        "    Example: 20.3°C -> 20.0-20.5 bucket, 20.7°C -> 20.5-21.0 bucket\n",
        "    Returns the lower bound of the bucket\n",
        "    \"\"\"\n",
        "    return np.floor(temp_celsius * 2) / 2\n",
        "\n",
        "def are_polygons_adjacent(geom1, geom2):\n",
        "    \"\"\"Check if two polygons are touching\"\"\"\n",
        "    return geom1.touches(geom2) or geom1.intersects(geom2)\n",
        "\n",
        "def merge_temperature_ranges(geojson_data, debug=True):\n",
        "    features = geojson_data['features']\n",
        "\n",
        "    # First, group features by temperature buckets\n",
        "    temp_buckets = {}\n",
        "    for feature in features:\n",
        "        temp = kelvin_to_celsius(float(feature['properties']['temperature']))\n",
        "        bucket = get_temp_bucket(temp)\n",
        "        if bucket not in temp_buckets:\n",
        "            temp_buckets[bucket] = []\n",
        "        temp_buckets[bucket].append(feature)\n",
        "\n",
        "    if debug:\n",
        "        print(\"\\nTemperature buckets:\")\n",
        "        for bucket in sorted(temp_buckets.keys()):\n",
        "            print(f\"{bucket}°C to {bucket+0.5}°C: {len(temp_buckets[bucket])} polygons\")\n",
        "\n",
        "    merged_features = []\n",
        "\n",
        "    # Process each temperature bucket\n",
        "    for bucket_temp in sorted(temp_buckets.keys()):\n",
        "        bucket_features = temp_buckets[bucket_temp]\n",
        "        if debug:\n",
        "            print(f\"\\nProcessing bucket {bucket_temp}°C to {bucket_temp+0.5}°C\")\n",
        "\n",
        "        # Keep track of which features in this bucket we've processed\n",
        "        processed = set()\n",
        "\n",
        "        # Find connected groups within each temperature bucket\n",
        "        for i, feature in enumerate(bucket_features):\n",
        "            if i in processed:\n",
        "                continue\n",
        "\n",
        "            current_geom = shape(feature['geometry'])\n",
        "            current_group = [feature]\n",
        "            processed.add(i)\n",
        "\n",
        "            # Find all adjacent polygons in the same temperature bucket\n",
        "            changed = True\n",
        "            while changed:\n",
        "                changed = False\n",
        "                for j, other_feature in enumerate(bucket_features):\n",
        "                    if j in processed:\n",
        "                        continue\n",
        "\n",
        "                    other_geom = shape(other_feature['geometry'])\n",
        "                    if are_polygons_adjacent(current_geom, other_geom):\n",
        "                        current_group.append(other_feature)\n",
        "                        current_geom = unary_union([current_geom, other_geom])\n",
        "                        processed.add(j)\n",
        "                        changed = True\n",
        "\n",
        "            # Create merged feature for this connected group\n",
        "            temps_kelvin = [float(f['properties']['temperature']) for f in current_group]\n",
        "            avg_temp_kelvin = np.mean(temps_kelvin)\n",
        "\n",
        "            if debug:\n",
        "                print(f\"  Created group of {len(current_group)} polygons\")\n",
        "\n",
        "            merged_feature = {\n",
        "                \"type\": \"Feature\",\n",
        "                \"properties\": {\n",
        "                    \"valid_time\": current_group[0]['properties']['valid_time'],\n",
        "                    \"temperature\": str(kelvin_to_celsius(avg_temp_kelvin)),\n",
        "                    \"latitude\": current_geom.centroid.y,\n",
        "                    \"longitude\": current_geom.centroid.x,\n",
        "                    \"merged_count\": len(current_group),\n",
        "                    \"temp_bucket\": f\"{bucket_temp:.1f}°C to {bucket_temp+0.5:.1f}°C\",\n",
        "                    \"lower_bound_temp\": f\"{bucket_temp:.1f}\",\n",
        "                    \"upper_bound_temp\": f\"{bucket_temp+0.5:.1f}\",\n",
        "                },\n",
        "                \"geometry\": mapping(current_geom)\n",
        "            }\n",
        "            merged_features.append(merged_feature)\n",
        "\n",
        "    output_geojson = {\n",
        "        \"type\": \"FeatureCollection\",\n",
        "        \"name\": geojson_data['name'],\n",
        "        \"crs\": geojson_data['crs'],\n",
        "        \"features\": merged_features\n",
        "    }\n",
        "\n",
        "    return output_geojson\n",
        "\n",
        "def process_geojson_file(input_file, output_file, debug=True):\n",
        "    \"\"\"\n",
        "    Process a GeoJSON file and save the merged result\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to input GeoJSON file\n",
        "        output_file (str): Path to save merged GeoJSON\n",
        "        debug (bool): Whether to print debug information\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r') as f:\n",
        "        geojson_data = json.load(f)\n",
        "\n",
        "    merged_geojson = merge_temperature_ranges(geojson_data, debug)\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(merged_geojson, f)\n",
        "\n",
        "    # Print reduction statistics\n",
        "    original_features = len(geojson_data['features'])\n",
        "    merged_features = len(merged_geojson['features'])\n",
        "    reduction = (1 - merged_features/original_features) * 100\n",
        "\n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"Original features: {original_features}\")\n",
        "    print(f\"Merged features: {merged_features}\")\n",
        "    print(f\"Reduction: {reduction:.1f}%\")\n",
        "    print(f\"Output File: {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "# process_geojson_file('temperature_20200101_000000.geojson',\n",
        "#                         'temperature_20200101_000000_merged.geojson')"
      ],
      "metadata": {
        "id": "XikxeFHXbVfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Dict, Any, Optional\n",
        "from shapely.geometry import shape, mapping\n",
        "from shapely.ops import unary_union\n",
        "import numpy as np\n",
        "\n",
        "def get_bucket(value: float, bucket_size: float = 0.5) -> float:\n",
        "    # Handle edge cases\n",
        "    if value < 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate bucket using floor division\n",
        "    bucket = (value // bucket_size) * bucket_size\n",
        "\n",
        "    return bucket\n",
        "\n",
        "def simplify_geojson(\n",
        "    geojson_data: Dict[str, Any],\n",
        "    bucket_key: str = 'temperature',\n",
        "    bucket_size: float = 0.5,\n",
        "    debug: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Simplify a GeoJSON by merging adjacent features in the same bucket.\n",
        "\n",
        "    Args:\n",
        "        geojson_data: Input GeoJSON dictionary\n",
        "        bucket_key: Property key to use for bucketing (default: 'temperature')\n",
        "        bucket_size: Size of bucket intervals (default: 0.5)\n",
        "        debug: Enable debug printing\n",
        "\n",
        "    Returns:\n",
        "        Simplified GeoJSON with merged features\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if not isinstance(geojson_data, dict):\n",
        "        raise ValueError(\"Input must be a GeoJSON dictionary\")\n",
        "\n",
        "    if 'features' not in geojson_data:\n",
        "        raise ValueError(\"Input must contain 'features' key\")\n",
        "\n",
        "    features = geojson_data['features']\n",
        "\n",
        "    if not features:\n",
        "        print(\"Warning: No features to process\")\n",
        "        return geojson_data\n",
        "\n",
        "    # Group features by bucket\n",
        "    buckets = {}\n",
        "    for feature in features:\n",
        "        try:\n",
        "            value = float(feature['properties'][bucket_key])\n",
        "            bucket = get_bucket(value)\n",
        "            if bucket not in buckets:\n",
        "                buckets[bucket] = []\n",
        "            buckets[bucket].append(feature)\n",
        "        except (KeyError, ValueError) as e:\n",
        "            print(f\"Warning: Skipping feature due to error: {e}\")\n",
        "\n",
        "    if debug:\n",
        "        print(\"\\nBucket distribution:\")\n",
        "        for bucket, bucket_features in buckets.items():\n",
        "            print(f\"Bucket {bucket}: {len(bucket_features)} polygons\")\n",
        "\n",
        "    # Merge features within each bucket\n",
        "    merged_features = []\n",
        "    for bucket, bucket_features in buckets.items():\n",
        "        processed = set()\n",
        "\n",
        "        for i, feature in enumerate(bucket_features):\n",
        "            if i in processed:\n",
        "                continue\n",
        "\n",
        "            current_geom = shape(feature['geometry'])\n",
        "            current_group = [feature]\n",
        "            processed.add(i)\n",
        "\n",
        "            # Find adjacent polygons\n",
        "            changed = True\n",
        "            while changed:\n",
        "                changed = False\n",
        "                for j, other_feature in enumerate(bucket_features):\n",
        "                    if j in processed:\n",
        "                        continue\n",
        "\n",
        "                    other_geom = shape(other_feature['geometry'])\n",
        "                    if current_geom.touches(other_geom) or current_geom.intersects(other_geom):\n",
        "                        current_group.append(other_feature)\n",
        "                        current_geom = unary_union([current_geom, other_geom])\n",
        "                        processed.add(j)\n",
        "                        changed = True\n",
        "\n",
        "            # Average properties for merged group\n",
        "            values = [float(f['properties'][bucket_key]) for f in current_group]\n",
        "            avg_value = np.mean(values)\n",
        "\n",
        "            # Create merged feature\n",
        "            merged_feature = {\n",
        "                \"type\": \"Feature\",\n",
        "                \"properties\": {\n",
        "                    # **current_group[0]['properties'],  # Preserve original properties\n",
        "                    bucket_key: str(avg_value),  # Update with average\n",
        "                    \"merged_count\": len(current_group),\n",
        "                    \"lower_bound\": bucket,\n",
        "                    \"upper_bound\": bucket + bucket_size,\n",
        "                    \"bucket_range\": f\"{bucket} to {bucket + bucket_size}\"\n",
        "                },\n",
        "                \"geometry\": mapping(current_geom)\n",
        "            }\n",
        "            merged_features.append(merged_feature)\n",
        "\n",
        "    # Create output GeoJSON\n",
        "    output_geojson = {\n",
        "        **geojson_data,  # Preserve original metadata\n",
        "        \"features\": merged_features\n",
        "    }\n",
        "\n",
        "    # Summary statistics\n",
        "    if debug:\n",
        "        original_count = len(features)\n",
        "        merged_count = len(merged_features)\n",
        "        reduction = (1 - merged_count/original_count) * 100\n",
        "        print(f\"\\nSimplification Summary:\")\n",
        "        print(f\"Original features: {original_count}\")\n",
        "        print(f\"Merged features: {merged_count}\")\n",
        "        print(f\"Reduction: {reduction:.1f}%\")\n",
        "\n",
        "    return output_geojson\n",
        "\n",
        "def process_geojson_file(\n",
        "    input_file: str,\n",
        "    output_file: str,\n",
        "    bucket_key: str = 'temperature',\n",
        "    bucket_size: float = 0.5,\n",
        "    debug: bool = True\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Process a GeoJSON file and save the simplified result.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to input GeoJSON file\n",
        "        output_file: Path to save simplified GeoJSON\n",
        "        bucket_key: Property key to use for bucketing\n",
        "        bucket_size: Size of bucket intervals\n",
        "        debug: Enable debug printing\n",
        "    \"\"\"\n",
        "    # Load input file\n",
        "    with open(input_file, 'r') as f:\n",
        "        geojson_data = json.load(f)\n",
        "\n",
        "    # Simplify GeoJSON\n",
        "    simplified_geojson = simplify_geojson(\n",
        "        geojson_data,\n",
        "        bucket_key=bucket_key,\n",
        "        bucket_size=bucket_size,\n",
        "        debug=debug\n",
        "    )\n",
        "\n",
        "    # Save output file\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(simplified_geojson, f, indent=2)\n"
      ],
      "metadata": {
        "id": "zTwWYe0gy_3v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a date object for jan 1 2020 00:00 and print out date like this 20200101_000000\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "date_object = datetime(2020, 5, 16, 0, 0)\n",
        "\n",
        "\n",
        "while date_object != datetime(2021, 5, 21, 0, 0):\n",
        "    formatted_input = date_object.strftime('content/processed/temperature_%Y%m%d_%H%M%S.geojson')\n",
        "    formatted_output = date_object.strftime('merged/temperature_%Y%m%d_%H%M%S_merged.geojson')\n",
        "    process_geojson_file(formatted_input, formatted_output, False)\n",
        "    date_object = date_object + timedelta(hours=6)"
      ],
      "metadata": {
        "id": "WWn7ei0UbYIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}